{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up input parameters\n",
    " \n",
    "Nobook is designed/Tested to run on IBM Datascience Plaotform (DSX), PowerAI and local installation, on each platfom input training **train_indessa.csv** data and test data **test_indessa** is stored \n",
    " 1. Object Storage in case of DSX \n",
    " 2. Nibmix file manager in case of Power AI on nimbix\n",
    " 3. Custome directory in case of Local\n",
    " \n",
    " **Below set of featured used**\n",
    " * Saving the model as check point at regular intervel\n",
    " * Creating Tensor Board logs for Network Analysis\n",
    " * Applied Mini Batch Gradient Descent\n",
    " * Traing data Standardization/Normalization\n",
    " * Learning rate decay\n",
    " * Continous graph of loss, train accuracy,, test accuracy and total accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs to the Neural Network\n",
    "\n",
    "1. **test_size** : splitting dataset in train and test, ginving size as .1 means train would be 90% and test would be 10%.\n",
    "2. layers_neurons_input = {\n",
    "    'L1': 300,\n",
    "    'L2': 200,\n",
    "    'L3': 100,}\n",
    "    \n",
    " L1-L3 are the layers and numbers in front is the number of neurons in each layer, user can add new layers say     L5, L6,L7\n",
    " \n",
    " For exmaple if user adds three new layers it shoud be like below\n",
    " \n",
    " layers_neurons_input = {\n",
    "    'L1': 500,\n",
    "    'L2': 400,\n",
    "    'L3': 300,\n",
    "    'L4': 200,\n",
    "    'L5': 100,\n",
    "    'L6': 50,\n",
    " }\n",
    "3. **batch_size**: Define the batch size for the Mini Batch Gradient Descent, pow(2,4) mean 16.\n",
    "4. **hm_epochs**: How many epocs/iteration for training.\n",
    "5. **activation_function** : Which activation function to use at output layer, all inner layers are using relu activation function\n",
    "    *softmax\n",
    "    *sigmoid\n",
    "6. **cost_function** : Define cost/loss function use either of three \n",
    "    *softmax_cross_entropy_with_logits\n",
    "    *reduce_sum\n",
    "    *RMSE\n",
    "7. **base_learning_rate** : Have used learning rate decay approach in learning rate decayed as the training progress, we have to define the starting/base learning rate to start from.\n",
    "8. **decay_rate** : Decay Rate\n",
    "9. **iterations_save_model**: No. of iteration to save the model.\n",
    "10. **no_of_training_records**: No of trainng records selected for traing 1000 mean top 1000 records will send for training, put -1 for all records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = .1\n",
    "layers_neurons_input ={\n",
    "    'L1': 175,\n",
    "    'L2': 135\n",
    "}\n",
    "batch_size = pow(2,7)\n",
    "hm_epochs = 200000\n",
    "activation_function = 'softmax' \n",
    "cost_function = 'softmax_cross_entropy_with_logits'        \n",
    "base_learning_rate = .0001\n",
    "decay_rate = .95\n",
    "iterations_save_model = 1000\n",
    "no_of_training_records = 20000 # -1 for all recods in the inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data load\n",
    "\n",
    "Update the correct file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "import pandas as pd\n",
    "file_path = '/data/CognitiveBanking/data/train_indessa.csv'\n",
    "dataframe = pd.read_csv(file_path)\n",
    "dataframe = dataframe.head(no_of_training_records)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating directory for storing Saved Model and Tensorboard logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from os.path import expanduser\n",
    "modelDir = expanduser(\"~\")+'/model'\n",
    "tbDir = expanduser(\"~\")+'/tb'\n",
    "\n",
    "if os.path.exists(modelDir):\n",
    "    shutil.rmtree(modelDir) \n",
    "    os.makedirs(modelDir)\n",
    "else:\n",
    "    os.makedirs(modelDir)\n",
    "print(os.listdir(modelDir))\n",
    "modelPath = modelDir+'/model.ckpt'\n",
    "\n",
    "\n",
    "if os.path.exists(tbDir):\n",
    "    shutil.rmtree(tbDir) \n",
    "    os.makedirs(tbDir)\n",
    "else:\n",
    "    os.makedirs(tbDir)\n",
    "tb_logs_path = tbDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is installation is required for first time run on POWER AI, as these libraries are not present by default.\n",
    "!pip install sklearn\n",
    "!pip install scipy \n",
    "\n",
    "#Splitting the data in train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "traindf, testdf = train_test_split(dataframe, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.loc[:, (\"y1\")] = dataframe.loc[:, (\"loan_status\")]\n",
    "dataframe.loc[:, (\"y2\")] = dataframe[\"y1\"] == 0           # y2 is the negation of y1\n",
    "dataframe.loc[:, (\"y2\")] = dataframe[\"y2\"].astype(int)    # Turn TRUE/FALSE values into 1/0\n",
    "\n",
    "\n",
    "traindf.loc[:, (\"y1\")] = traindf.loc[:, (\"loan_status\")]\n",
    "traindf.loc[:, (\"y2\")] = traindf[\"y1\"] == 0           # y2 is the negation of y1\n",
    "traindf.loc[:, (\"y2\")] = traindf[\"y2\"].astype(int)    # Turn TRUE/FALSE values into 1/0\n",
    "\n",
    "\n",
    "testdf.loc[:, (\"y1\")] = testdf.loc[:, (\"loan_status\")]\n",
    "testdf.loc[:, (\"y2\")] = testdf[\"y1\"] == 0           # y2 is the negation of y1\n",
    "testdf.loc[:, (\"y2\")] = testdf[\"y2\"].astype(int)    # Turn TRUE/FALSE values into 1/0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This mthod is Standardizing the inputs to zero mean between 0-1\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def min_max_scale_normalization(df):\n",
    "    scaler = MinMaxScaler()\n",
    "    return pd.DataFrame(scaler.fit_transform(df), columns=df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter the required columns\n",
    "Create three datasets, **train**,**test** and **complete** datasets with required features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataframe.loc[:, ['loan_amnt',\n",
    "                           'funded_amnt',\n",
    "                           'funded_amnt_inv', \n",
    "                           'int_rate',\n",
    "                           'annual_inc', \n",
    "                           'dti',\n",
    "                           'open_acc', \n",
    "                           'revol_bal',\n",
    "                           'revol_util', \n",
    "                           'total_acc',\n",
    "                           'total_rec_int', \n",
    "                           'tot_cur_bal',\n",
    "                           'total_rev_hi_lim',\n",
    "                           'y1',\n",
    "                           'y2']].dropna()\n",
    "\n",
    "\n",
    "#df = min_max_scale_normalization(df)\n",
    "\n",
    "inputX = df.loc[:, ['loan_amnt',\n",
    "                           'funded_amnt',\n",
    "                           'funded_amnt_inv', \n",
    "                           'int_rate',\n",
    "                           'annual_inc', \n",
    "                           'dti',\n",
    "                           'open_acc', \n",
    "                           'revol_bal',\n",
    "                           'revol_util', \n",
    "                           'total_acc',\n",
    "                           'total_rec_int', \n",
    "                           'tot_cur_bal',\n",
    "                           'total_rev_hi_lim'\n",
    "                           ]].as_matrix()\n",
    "\n",
    "inputY = df.loc[:, [\"y1\",\"y2\"]].as_matrix()\n",
    "\n",
    "print('Complete Dataset')\n",
    "print(inputX.shape)\n",
    "print(inputY.shape)\n",
    "print('\\n')\n",
    "\n",
    "traindf = traindf.loc[:, ['loan_amnt',\n",
    "                           'funded_amnt',\n",
    "                           'funded_amnt_inv', \n",
    "                           'int_rate',\n",
    "                           'annual_inc', \n",
    "                           'dti',\n",
    "                           'open_acc', \n",
    "                           'revol_bal',\n",
    "                           'revol_util', \n",
    "                           'total_acc',\n",
    "                           'total_rec_int', \n",
    "                           'tot_cur_bal',\n",
    "                           'total_rev_hi_lim',\n",
    "                           'y1',\n",
    "                           'y2']].dropna()\n",
    "\n",
    "\n",
    "#traindf = min_max_scale_normalization(traindf)\n",
    "\n",
    "inputXtrain = traindf.loc[:, ['loan_amnt',\n",
    "                           'funded_amnt',\n",
    "                           'funded_amnt_inv', \n",
    "                           'int_rate',\n",
    "                           'annual_inc', \n",
    "                           'dti',\n",
    "                           'open_acc', \n",
    "                           'revol_bal',\n",
    "                           'revol_util', \n",
    "                           'total_acc',\n",
    "                           'total_rec_int', \n",
    "                           'tot_cur_bal',\n",
    "                           'total_rev_hi_lim'\n",
    "                           ]].as_matrix()\n",
    "\n",
    "inputYtrain = traindf.loc[:, [\"y1\",\"y2\"]].as_matrix()\n",
    "print('Traning Dataset')\n",
    "print(inputXtrain.shape)\n",
    "print(inputYtrain.shape)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "testdf = testdf.loc[:, ['loan_amnt',\n",
    "                           'funded_amnt',\n",
    "                           'funded_amnt_inv', \n",
    "                           'int_rate',\n",
    "                           'annual_inc', \n",
    "                           'dti',\n",
    "                           'open_acc', \n",
    "                           'revol_bal',\n",
    "                           'revol_util', \n",
    "                           'total_acc',\n",
    "                           'total_rec_int', \n",
    "                           'tot_cur_bal',\n",
    "                           'total_rev_hi_lim',\n",
    "                           'y1',\n",
    "                           'y2']].dropna()\n",
    "\n",
    "#testdf = min_max_scale_normalization(testdf)\n",
    "\n",
    "\n",
    "inputXtest = testdf.loc[:, ['loan_amnt',\n",
    "                           'funded_amnt',\n",
    "                           'funded_amnt_inv', \n",
    "                           'int_rate',\n",
    "                           'annual_inc', \n",
    "                           'dti',\n",
    "                           'open_acc', \n",
    "                           'revol_bal',\n",
    "                           'revol_util', \n",
    "                           'total_acc',\n",
    "                           'total_rec_int', \n",
    "                           'tot_cur_bal',\n",
    "                           'total_rev_hi_lim'\n",
    "                           ]].as_matrix()\n",
    "\n",
    "inputYtest = testdf.loc[:, [\"y1\",\"y2\"]].as_matrix()\n",
    "\n",
    "print('Test Dataset')\n",
    "print(inputXtest.shape)\n",
    "print(inputYtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "\n",
    "#[0,1]\n",
    "#with tf.name_scope('Input'):\n",
    "X = tf.placeholder('float', [None, 13],name='X_Input')\n",
    "Y_ = tf.placeholder('float', [None, 2],name='Y_Input')\n",
    "\n",
    "\n",
    "\n",
    "no_of_nodes = {\n",
    "    'L0': inputXtrain.shape[1],\n",
    "}\n",
    "no_of_nodes.update(layers_neurons_input)\n",
    "no_of_nodes.update({'L'+str(len(layers_neurons_input)+1):inputYtrain.shape[1]})\n",
    "\n",
    "\n",
    "no_of_layers = len(no_of_nodes)-1\n",
    "W = {}\n",
    "b = {}\n",
    "Y = {}\n",
    "\n",
    "#with tf.name_scope('Network'):   \n",
    "for i in range(no_of_layers):\n",
    "    W[i+1] = tf.Variable(tf.truncated_normal([no_of_nodes['L'+str(i)], no_of_nodes['L'+str(i+1)]], stddev=0.1),name='Weight_L'+str(i+1))\n",
    "    b[i+1] = tf.Variable(tf.zeros([no_of_nodes['L'+str(i+1)]]),name='Bias_L'+str(i+1))\n",
    "\n",
    "    if i==0:\n",
    "        Y[i+1] = tf.nn.relu(tf.matmul(X, W[i+1]) + b[i+1],name='Relu_L'+str(i+1))\n",
    "    elif i==no_of_layers-1:\n",
    "        output = tf.matmul(Y[i], W[i+1]) + b[i+1]\n",
    "\n",
    "        if activation_function == 'sigmoid':\n",
    "            Y[i+1] = tf.nn.sigmoid(output,name='Y_Output')\n",
    "        elif activation_function == 'softmax':\n",
    "            Y[i+1] = tf.nn.softmax(output,name='Y_Output')#name='Softmax_L'+str(i+1),\n",
    "    else:\n",
    "        Y[i+1] = tf.nn.relu(tf.matmul(Y[i], W[i+1]) + b[i+1],name='Relu_L'+str(i+1))\n",
    "\n",
    "    \n",
    "#DO NOT DELETE, BELOW IS THE EXPANDED VIEW OF THE ABOVE DYNAMIC CREATION OF NETWORK LAYER\n",
    "# layers sizes\n",
    "#L0 = 13\n",
    "#L1 = 500\n",
    "#L2 = 500\n",
    "#L3 = 500\n",
    "#L4 = 500\n",
    "#L5 = 2\n",
    "\n",
    "# weights - initialized with random values from normal distribution mean=0, stddev=0.1\n",
    "# output of one layer is input for the next\n",
    "#W1 = tf.Variable(tf.truncated_normal([L0, L1], stddev=0.1))\n",
    "#b1 = tf.Variable(tf.zeros([L1]),name=\"b1\")\n",
    "\n",
    "#W2 = tf.Variable(tf.truncated_normal([L1, L2], stddev=0.1))\n",
    "#b2 = tf.Variable(tf.zeros([L2]),name=\"b2\")\n",
    "\n",
    "#W3 = tf.Variable(tf.truncated_normal([L2, L3], stddev=0.1))\n",
    "#b3 = tf.Variable(tf.zeros([L3]),name=\"b3\")\n",
    "\n",
    "#W4 = tf.Variable(tf.truncated_normal([L3, L4], stddev=0.1))\n",
    "#b4 = tf.Variable(tf.zeros([L4]),name=\"b4\")\n",
    "\n",
    "#W5 = tf.Variable(tf.truncated_normal([L4, L5], stddev=0.1))\n",
    "#b5 = tf.Variable(tf.zeros([L5]),name=\"b5\")\n",
    "\n",
    "  \n",
    "\n",
    "# Define model\n",
    "#Y1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "#Y2 = tf.nn.relu(tf.matmul(Y1, W2) + b2)\n",
    "#Y3 = tf.nn.relu(tf.matmul(Y2, W3) + b3)\n",
    "#Y4 = tf.nn.relu(tf.matmul(Y3, W4) + b4)\n",
    "#output = tf.matmul(Y4, W5) + b5\n",
    "#Y = tf.nn.sigmoid(output,name=\"Y\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to create new btach\n",
    "This is used for mini batches in mini batch Gradient Descent, Batach size decides type of Gradient Descent\n",
    "1. if batch_size = 1 then its Stochastic Gradient descent\n",
    "2. If batch_size = pow(2,n) then its Mini batch Gradient descent\n",
    "3. If batch_size = inputXtrain.shape[0] then its Btach Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = inputXtrain.shape[0]\n",
    "def nextBatch(inputArray,batchCount):\n",
    "    start = batchCount*batch_size\n",
    "    end = start + batch_size\n",
    "    return inputArray[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Plot Functions\n",
    "Includes function to plot graph for\n",
    "1. Loss w.r.y epocs\n",
    "2. Train and Test accuracy w.r.t epocs\n",
    "3. Total Accuracy w.r.t epocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from pylab import *\n",
    "%matplotlib inline \n",
    "\n",
    "\n",
    "def lossGraph(epochCount):\n",
    "    from matplotlib import pyplot as plt1\n",
    "    plt1.figure(figsize=(20,5))\n",
    "    plt1.plot(lossList,label='Training Loss')\n",
    "    axes = plt1.gca()\n",
    "    axes.set_xlim(0,epochCount)\n",
    "    #plt1.text(.8,.8,'Epoch Count '+str(epochCount))\n",
    "    plt1.xlabel('No of Epocs')\n",
    "    plt1.ylabel('Training Loss')\n",
    "    plt1.title('Trainig Loss with Epocs') \n",
    "    plt1.grid()\n",
    "    plt1.show()\n",
    "    \n",
    "def accuracyGraph(epochCount):\n",
    "    from matplotlib import pyplot as plt2\n",
    "    plt2.figure(figsize=(20,5))\n",
    "    plt2.plot(trainAccuracyList, color='red', label='Train Accuracy')\n",
    "    plt2.plot(testAccuracyList, color='blue', label='Test Accuracy')\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([0,100])\n",
    "    axes.set_xlim(0,epochCount)\n",
    "    #plt2.text(.8,.8,'Epoch Count '+str(epochCount))\n",
    "    plt2.xlabel('No of Epocs')\n",
    "    plt2.ylabel('Accuracy')\n",
    "    plt2.title('Accuracy with Epocs') \n",
    "    plt2.grid()\n",
    "    plt2.show()\n",
    "\n",
    "percentMatch = [0]\n",
    "def testModel(epochCount):\n",
    "    clssification = sess.run(Y[no_of_layers], feed_dict={X: inputX})\n",
    "    j=0\n",
    "    for i in range(clssification.shape[0]):\n",
    "        if clssification[i].argmax() == inputY[i].argmax():\n",
    "            j=j+1\n",
    "            #graph.append(1)\n",
    "        #else:\n",
    "            #graph.append(0)\n",
    "    if(clssification.shape[0] >0):\n",
    "        percentMatch.append((j/clssification.shape[0])*100)\n",
    "        print ('Total Accuracy',(j/clssification.shape[0])*100)\n",
    "        plt.figure(figsize=(20,5))\n",
    "        plt.plot(percentMatch)\n",
    "        axes = plt.gca()\n",
    "        axes.set_ylim([0,100])\n",
    "        axes.set_xlim(left=0)\n",
    "        #plt.text(.8,.8,'Epoch Count '+str(epochCount))\n",
    "        plt.xlabel('No of Epocs')\n",
    "        plt.ylabel('Total Accuracy')\n",
    "        plt.title('Accuracy with Epocs') \n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "print('Training started with below inputs')\n",
    "print('Test Size              :',test_size)\n",
    "print('Layers and Neurons     :',layers_neurons_input)\n",
    "print('Batch Size             :',batch_size)\n",
    "print('Number of Epocs        :',hm_epochs)\n",
    "print('Activation Function    :',activation_function)\n",
    "print('Cost Function          :',cost_function)\n",
    "print('Base Learning Rate     :',base_learning_rate)\n",
    "print('Decay Rate             :',decay_rate)\n",
    "print('Iteration Save Model   :',iterations_save_model)\n",
    "print('No of Training Records :',no_of_training_records)\n",
    "print('\\n')\n",
    "\n",
    "# reset everything to rerun in jupyter\n",
    "#tf.reset_default_graph()\n",
    "\n",
    "# Optimizer: set up a variable that's incremented once per batch and\n",
    "# controls the learning rate decay.\n",
    "batch = tf.Variable(0)\n",
    "#learning_rate = tf.train.exponential_decay(\n",
    " # base_learning_rate,                # Base learning rate.\n",
    "  #batch * batch_size,  # Current index into the dataset.\n",
    "  #inputXtrain.shape[0],# Decay step,train_size.\n",
    "  #decay_rate,                # Decay rate.\n",
    "  #staircase=True)\n",
    "\n",
    "with tf.name_scope('Cost'):  \n",
    "    if cost_function == 'softmax_cross_entropy_with_logits':\n",
    "        cross_entropy = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=Y_),name='cross_entropy')\n",
    "    elif cost_function == 'reduce_sum':\n",
    "        cross_entropy = -tf.reduce_sum(output * tf.log(Y_))\n",
    "    elif cost_function == 'RMSE':\n",
    "        cross_entropy = tf.sqrt(tf.reduce_mean(tf.squared_difference(Y_[0],output[0])))\n",
    "with tf.name_scope('Optimizer'):     \n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy,global_step = batch)\n",
    "    optimizer = tf.train.AdamOptimizer(.001).minimize(cross_entropy)\n",
    "\n",
    "\n",
    "# accuracy of the trained model, between 0 (worst) and 1 (best)\n",
    "with tf.name_scope('Correct_Prdiction'):     \n",
    "    correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(Y_, 1))\n",
    "\n",
    "with tf.name_scope('Accuracy'):     \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32),name='Accuracy')*100\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "config = tf.ConfigProto(log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config) \n",
    "sess.run(tf.global_variables_initializer())\n",
    "trainAccuracyList = list()\n",
    "testAccuracyList = list()\n",
    "lossList = list()\n",
    "\n",
    "\n",
    "#saver.restore(sess,tf.train.latest_checkpoint(modelDir+'/.'))\n",
    "\n",
    "# Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "#logs_path = 'C:/Arshad/Cognitive Banking/Training Data/Tensor Board'\n",
    "# create a summary for our cost and accuracy\n",
    "#tf.summary.scalar(\"Cost\", cost)\n",
    "#tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "\n",
    "# merge all summaries into a single \"operation\" which we can execute in a session \n",
    "#summary_op = tf.summary.merge_all()\n",
    "\n",
    "#merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(tb_logs_path,sess.graph)\n",
    "\n",
    "trainingStart = datetime.datetime.now()\n",
    "timeStart = datetime.datetime.now()\n",
    "\n",
    "for epoch in range(hm_epochs):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    batch_count = int(inputXtrain.shape[0]/batch_size)\n",
    "    for i in range(batch_count):\n",
    "        epoch_x, epoch_y =  nextBatch(inputXtrain,i),  nextBatch(inputYtrain,i)\n",
    "        #epoch_x, epoch_y =  inputXtrain,  inputYtrain\n",
    "        _ , c = sess.run([optimizer,cross_entropy], feed_dict={X: epoch_x, Y_: epoch_y})\n",
    "        epoch_loss += c   \n",
    "        #train_writer.add_summary(summary, epoch =* batch_count + i)\n",
    "\n",
    "    trainAcc = sess.run([accuracy], feed_dict={X: inputXtrain, Y_: inputYtrain})\n",
    "    testAcc = sess.run([accuracy], feed_dict={X: inputXtest, Y_: inputYtest})\n",
    "    lossList.append(epoch_loss/100)\n",
    "    trainAccuracyList.append(trainAcc)\n",
    "    testAccuracyList.append(testAcc)\n",
    "\n",
    "    \n",
    "    #print('Epoch', epoch+1, 'completed out of',hm_epochs,'loss:',epoch_loss, 'in:',executionTime,'seconds Total training time elapsed:',timeEnd-trainingStart)\n",
    "    #print('Train Accuracy: ',trainAcc,' Test Accuracy:',testAcc)\n",
    "    \n",
    "    if (epoch+1)%iterations_save_model==0:\n",
    "        timeEnd = datetime.datetime.now()\n",
    "        executionTime = timeEnd - timeStart\n",
    "        timeStart = datetime.datetime.now()\n",
    "        print('Epoch', epoch+1, 'completed out of',hm_epochs,'loss:',epoch_loss, 'in:',executionTime,'seconds Total training time elapsed:',timeEnd-trainingStart)\n",
    "        print('Train Accuracy: ',trainAcc,' Test Accuracy:',testAcc)\n",
    "        lossGraph(epoch+1)\n",
    "        accuracyGraph(epoch+1)\n",
    "        testModel(epoch+1)\n",
    "        save_path = saver.save(sess, modelPath,global_step=epoch+1)\n",
    "        print(\"Model saved in file: %s\" % save_path) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
